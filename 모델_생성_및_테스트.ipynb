{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_creova  :  ./train/10_creova\\KakaoTalk_20210911_164006583_01.jpg\n",
      "10_creova  :  ./train/10_creova\\KakaoTalk_20210911_164006583_21.jpg\n",
      "1_ADVIL  :  ./train/1_ADVIL\\a2.jpg\n",
      "1_ADVIL  :  ./train/1_ADVIL\\dp_0_0_259.jpg\n",
      "1_ADVIL  :  ./train/1_ADVIL\\dp_0_0_5051.jpg\n",
      "1_ADVIL  :  ./train/1_ADVIL\\dp_0_0_6954.jpg\n",
      "1_ADVIL  :  ./train/1_ADVIL\\dp_0_0_9422.jpg\n",
      "2_DP  :  ./train/2_DP\\dp.jpg\n",
      "2_DP  :  ./train/2_DP\\dp_0_0_294.jpg\n",
      "2_DP  :  ./train/2_DP\\dp_0_0_5574.jpg\n",
      "2_DP  :  ./train/2_DP\\dp_0_0_7179.jpg\n",
      "2_DP  :  ./train/2_DP\\dp_0_0_9696.jpg\n",
      "3_SUSPEN  :  ./train/3_SUSPEN\\s10.jpg\n",
      "3_SUSPEN  :  ./train/3_SUSPEN\\s29.jpg\n",
      "3_SUSPEN  :  ./train/3_SUSPEN\\s47.jpg\n",
      "3_SUSPEN  :  ./train/3_SUSPEN\\s65.jpg\n",
      "4_beaze  :  ./train/4_beaze\\KakaoTalk_20210911_162408561 (1).jpg\n",
      "4_beaze  :  ./train/4_beaze\\KakaoTalk_20210911_162408561_10.jpg\n",
      "4_beaze  :  ./train/4_beaze\\KakaoTalk_20210911_162408561_22.jpg\n",
      "5_imfectamin  :  ./train/5_imfectamin\\KakaoTalk_20210828_215606307.jpg\n",
      "5_imfectamin  :  ./train/5_imfectamin\\KakaoTalk_20210828_215606307_20.jpg\n",
      "6_Centrum  :  ./train/6_Centrum\\1.jpg\n",
      "6_Centrum  :  ./train/6_Centrum\\28.jpg\n",
      "7_kelolef  :  ./train/7_kelolef\\KakaoTalk_20210911_162500872.jpg\n",
      "7_kelolef  :  ./train/7_kelolef\\KakaoTalk_20210911_162500872_10.jpg\n",
      "8_selfelinF  :  ./train/8_selfelinF\\KakaoTalk_20210911_162544205 (1).jpg\n",
      "8_selfelinF  :  ./train/8_selfelinF\\KakaoTalk_20210911_162544205_10.jpg\n",
      "9_mekain  :  ./train/9_mekain\\KakaoTalk_20210911_163015845.jpg\n",
      "9_mekain  :  ./train/9_mekain\\KakaoTalk_20210911_163015845_21.jpg\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 132,873,346\n",
      "Trainable params: 132,873,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "caltech_dir = \"./train\"\n",
    "label_name_list = []\n",
    "data_list = glob('train/*/*.jpg')\n",
    "\n",
    "def get_label_from_path(path):\n",
    "    return path.split('\\\\')[-2]\n",
    "\n",
    "for path in data_list:\n",
    "    label_name_list.append(get_label_from_path(path))\n",
    "categories = np.unique(label_name_list)\n",
    "nb_classes = len(categories)\n",
    "\n",
    "image_w = 224\n",
    "image_h = 224\n",
    "\n",
    "all_image_paths = []\n",
    "all_onehot_labels = []\n",
    "\n",
    "for idx, v in enumerate(categories):\n",
    "    # 레이블 지정\n",
    "    label = [0 for i in range(nb_classes)]  # one-hot준비 [0,0,0,0,0]\n",
    "    label[idx] = 1   # one-hot 리스트 생성\n",
    "    # 각 폴더에 있는 모든 파일이름에 대한 리스트 생성\n",
    "    files = glob(caltech_dir + \"/\" + categories[idx] + \"/*.jpg\")\n",
    "    for i,f in enumerate(files):\n",
    "        all_image_paths.append(f)\n",
    "        all_onehot_labels.append(label)\n",
    "        \n",
    "        if i%20 == 0:\n",
    "             print(v, \" : \", f)\n",
    "        \n",
    "def load_image_path_label(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [image_w,image_h])\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    return image, label\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices((all_image_paths, all_onehot_labels))\n",
    "full_dataset = full_dataset.map(load_image_path_label)        \n",
    "\n",
    "DATASET_SIZE = len(all_image_paths)\n",
    "\n",
    "train_size = int(0.75 * DATASET_SIZE)\n",
    "test_size = DATASET_SIZE - train_size\n",
    "# 랜덤하게 shuffling\n",
    "full_dataset = full_dataset.shuffle(buffer_size = int(DATASET_SIZE*1.5))\n",
    "\n",
    "# 학습 데이터 생성\n",
    "train_ds = full_dataset.take(train_size)\n",
    "train_ds = train_ds.batch(16)\n",
    "\n",
    "# 나머지를 테스트 용으로 사용\n",
    "test_ds = full_dataset.skip(train_size)\n",
    "test_ds = test_ds.batch(16)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size=(3, 3), input_shape=(image_w, image_h, 3), padding=\"same\", activation='relu'))\n",
    "# model.add(Conv2D(filters = 64, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # default\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "# model.add(Conv2D(filters = 128, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(filters = 256, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(Conv2D(filters = 256, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "# model.add(Conv2D(filters = 256, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(filters = 512, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(Conv2D(filters = 512, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "# model.add(Conv2D(filters = 512, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(filters = 512, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(Conv2D(filters = 512, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "# model.add(Conv2D(filters = 512, kernel_size=(3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "'''        \n",
    "model.compile(optimizer = \"NAdam\",\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "'''\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', \n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "'''\n",
    "model_dir = './model'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "        \n",
    "model_path = model_dir + '{epoch}.model'\n",
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
    "'''\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 73s 3s/step - loss: 4.2397 - accuracy: 0.1811\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 77s 3s/step - loss: 1.8028 - accuracy: 0.3287\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 80s 3s/step - loss: 1.6346 - accuracy: 0.3454\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 1.4902 - accuracy: 0.3426\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 77s 3s/step - loss: 1.1451 - accuracy: 0.5905\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 1.6297 - accuracy: 0.4791\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.9141 - accuracy: 0.6351\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.7386 - accuracy: 0.7019\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 1.2818 - accuracy: 0.5460\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.6830 - accuracy: 0.6992\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 1.2100 - accuracy: 0.6546\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.7214 - accuracy: 0.7326\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 73s 3s/step - loss: 0.7916 - accuracy: 0.7159\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 74s 3s/step - loss: 0.5563 - accuracy: 0.7604\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 77s 3s/step - loss: 0.4998 - accuracy: 0.7911\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.3543 - accuracy: 0.8719\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.4180 - accuracy: 0.8607\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.4096 - accuracy: 0.8496\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.3986 - accuracy: 0.8440\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.2456 - accuracy: 0.9164\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.5674 - accuracy: 0.8245\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.2867 - accuracy: 0.8830\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 77s 3s/step - loss: 0.3096 - accuracy: 0.8942\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 77s 3s/step - loss: 0.2575 - accuracy: 0.8942\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 1.5267 - accuracy: 0.6156\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 1.0252 - accuracy: 0.6964\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.6461 - accuracy: 0.7437\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.6476 - accuracy: 0.7660\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.3383 - accuracy: 0.8691\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.2004 - accuracy: 0.9220\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.1868 - accuracy: 0.9387\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.2033 - accuracy: 0.9248\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.1492 - accuracy: 0.9443\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.5311 - accuracy: 0.8468\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 79s 3s/step - loss: 0.9323 - accuracy: 0.7577\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 2.3196 - accuracy: 0.5543\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.7074 - accuracy: 0.7799\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.4186 - accuracy: 0.8273\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.4325 - accuracy: 0.8886\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 74s 3s/step - loss: 0.6838 - accuracy: 0.8217\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 74s 3s/step - loss: 0.3128 - accuracy: 0.8607\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 74s 3s/step - loss: 0.3757 - accuracy: 0.8830\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.1827 - accuracy: 0.9387\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 73s 3s/step - loss: 0.1450 - accuracy: 0.9471\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 73s 3s/step - loss: 0.0584 - accuracy: 0.9861\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 74s 3s/step - loss: 0.0661 - accuracy: 0.9749\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.2768 - accuracy: 0.9248\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 75s 3s/step - loss: 0.1908 - accuracy: 0.9499\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 72s 3s/step - loss: 0.1115 - accuracy: 0.9638\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 73s 3s/step - loss: 0.0537 - accuracy: 0.9777\n",
      "8/8 [==============================] - 8s 611ms/step - loss: 0.1032 - accuracy: 0.9750\n",
      "INFO:tensorflow:Assets written to: test10_img_classification.model\\assets\n"
     ]
    }
   ],
   "source": [
    "#model.evaluate(test_ds)\n",
    "model.fit(train_ds, epochs=50)\n",
    "\n",
    "model.evaluate(test_ds)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model.save('test10_img_classification.model')\n",
    "\n",
    "#history = model.fit(train_ds, epochs=20, callbacks=[checkpoint, early_stopping])\n",
    "#model.fit(train_ds, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000]\n",
      "1\n",
      "해당 advil.jpg이미지는 DP로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000]\n",
      "2\n",
      "해당 beaze.jpg이미지는 SUSPEN로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000]\n",
      "2\n",
      "해당 centrom.jpg이미지는 SUSPEN로 추정됩니다.\n",
      "[1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 crerova.jpg이미지는 ADVIL로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000]\n",
      "2\n",
      "해당 dp.jpg이미지는 SUSPEN로 추정됩니다.\n",
      "[0.000 0.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000]\n",
      "4\n",
      "해당 impactamin.jpg이미지는 IMFECTAMIN로 추정됩니다.\n",
      "[0.000 0.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000]\n",
      "4\n",
      "해당 kalolef.jpg이미지는 IMFECTAMIN로 추정됩니다.\n",
      "[0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000]\n",
      "1\n",
      "해당 mekain.jpg이미지는 DP로 추정됩니다.\n",
      "[0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000]\n",
      "1\n",
      "해당 selferinF.jpg이미지는 DP로 추정됩니다.\n",
      "[0.000 0.000 0.000 1.000 0.000 0.000 0.000 0.000 0.000 0.000]\n",
      "3\n",
      "해당 suspen.jpg이미지는 beaze로 추정됩니다.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('./model/test10_img_classification.model') # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('./model/model_9.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "test_dir = './test'\n",
    "image_w = 224\n",
    "image_h = 224\n",
    "\n",
    "X= []\n",
    "filenames = []\n",
    "files = glob.glob(test_dir+'/*.jpg')\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    filenames.append(f)\n",
    "    X.append(data)\n",
    "\n",
    "X = np.array(X)\n",
    "model = load_model('./model/test10_img_classification.model')\n",
    "\n",
    "prediction = model.predict(X)\n",
    "np.set_printoptions(formatter={'float':lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0\n",
    "\n",
    "result = prediction\n",
    "result\n",
    "\n",
    "\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"ADVIL\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"DP\"\n",
    "    elif pre_ans == 2: pre_ans_str = \"SUSPEN\"\n",
    "    elif pre_ans == 3: pre_ans_str = \"beaze\"\n",
    "    elif pre_ans == 4: pre_ans_str = \"IMFECTAMIN\"\n",
    "    elif pre_ans == 5: pre_ans_str = \"CENTRUM\"\n",
    "    elif pre_ans == 6: pre_ans_str = \"kelolef\"\n",
    "    elif pre_ans == 7: pre_ans_str = \"selfelinF\"\n",
    "    elif pre_ans == 8: pre_ans_str = \"mekain\"\n",
    "    else: pre_ans_str = \"creova\"\n",
    "\n",
    "    if i[0] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[4] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[5] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[6] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[7] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[8] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[9] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "   \n",
    "    \n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
